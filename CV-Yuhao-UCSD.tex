\documentclass[margin,line]{res}
\usepackage[all]{nowidow}



\oddsidemargin -.5in
\evensidemargin -.5in
\textwidth=6.0in
\itemsep=0in
\parsep=0in
% if using pdflatex:
%\setlength{\pdfpagewidth}{\paperwidth}
%\setlength{\pdfpageheight}{\paperheight} 

\newenvironment{list1}{
 \begin{list}{\ding{113}}{%
   \setlength{\itemsep}{0in}
   \setlength{\parsep}{0in} \setlength{\parskip}{0in}
   \setlength{\topsep}{0in} \setlength{\partopsep}{0in} 
   \setlength{\leftmargin}{0.17in}}}{\end{list}}
\newenvironment{list2}{
 \begin{list}{$\bullet$}{%
   \setlength{\itemsep}{0in}
   \setlength{\parsep}{0in} \setlength{\parskip}{0in}
   \setlength{\topsep}{0in} \setlength{\partopsep}{0in} 
   \setlength{\leftmargin}{0.2in}}}{\end{list}}


\begin{document}\sloppy

\name{Yuhao Zhang \vspace*{.1in}}

\begin{resume}
\section{\sc Contact Information}
\vspace{.05in}
\begin{tabular}{@{}p{2in}p{4in}}
& {\it E-mail:} yhzhang1994@gmail.com, yuhao.zhang@databricks.com\\
& {\it Website:} https://yhzhang.info/ \\     
\end{tabular}

\section{\sc Professional Experience}
{\bf Databricks}, Mountain View, California USA

\vspace{-.3cm}
{\em Software Engineer} \hfill {\bf December 2023 - present}\\
Currently, I drive key research initiatives on applied AI/ML for systems in Databricks, applying state-of-the-art machine learning techniques to advance and optimize Databricks' data platform. My role involves developing, prototyping, and deploying innovative solutions in areas such as system scheduling and workload management, directly benefiting the performance and intelligence of Databricks products, and advancing industry standards for data-driven technologies.

{\bf Microsoft Gray System Lab}, California USA

\vspace{-.3cm}
{\em Research Intern} \hfill {\bf Summer 2021}\\
Conducted research in scalable machine learning for database management systems, focusing on factorized in-DBMS machine learning. Developed a highly scalable system atop Apache Spark, surpassing state-of-the-art systems by orders of magnitude, enabling efficient ML operations for large-scale data.

{\bf VMware}, Palo Alto, California USA

\vspace{-.3cm}
{\em Software Engineer Intern} \hfill {\bf Summer 2019}\\
Led the development of the first in-DBMS deep learning system to support training and inference of TensorFlow models on database-resident data. My research project, Cerebro, was integrated into VMware's Greenplum Database, achieving over a 10x boost in efficiency for deep learning operations and contributing to the Apache MADlib project. This project has been incorporated into VMware's Greenplum product, enhancing its capabilities for customers across various sectors and solidifying Greenplum's standing as an advanced data system for machine learning.

{\bf Opera Solutions}, San Diego, California USA

\vspace{-.3cm}
{\em Data Scientist Intern} \hfill {\bf Summer 2018}\\
Worked on a scheduling and recommendation system, proposing new models and optimizations to enhance performance and functionality.

\section{\sc Education}
{\bf University of California, San Diego}, La Jolla, California USA\\
%{\em Department of Statistics} 
\vspace*{-.1in}
\begin{list1}
\item[] Ph.D., Computer Science, November 2023
\item[] M.S., Computer Science, 2021
\begin{list2}
\vspace*{.05in}
%\item Dissertation Topic: ``Hierarchical Models for Multiple Ratings
% in Performance-Based\\ \hspace*{1.23in} Student Assessments.'' 
\item Dissertation: High-throughput Data Systems for Deep Learning Workloads
\item Advisor: Prof. Arun Kumar
\end{list2}
\end{list1}

{\bf Nankai University}, Tianjin, China\\
\vspace*{-.1in}
\begin{list1}
\item[] B.S., Theoretical Physics, June 2016
\begin{list2}
\item Advisor: Prof. Xueqian Li and Prof. Yangang Miao
\end{list2}
\end{list1}


\section{\sc Research Interests and Impact}
My research centers on advancing ML systems and large-scale data analytics, focusing on both applied ML systems that power novel applications and ML-optimized systems that streamline data science workflows for efficiency and scalability. My past and ongoing projects span large-scale distributed AutoML, distributed in-database deep learning, scalable graph neural network training, and advanced video analytics and querying systems. All previous work has been released as open-source software. My research on Cerebro and Cerebro-DS has been incorporated into the Apache MADlib open-source project and integrated into VMware’s Greenplum Database, directly enhancing its capabilities for enterprise customers and enabling public health researchers at UCSD to train ML/AI models for understanding and preventing kidney and digestive diseases. Additionally, these projects are available as third-party tools for Databricks users, broadening access and impact across data science teams. My work has also inspired the creation of RapidFire AI, an AI startup that recently raised \$4M in seed funding based on my research contributions. Furthermore, my work on Lotan, a system for large-scale GNN training, has garnered interest from a prominent graph DBMS vendor, underscoring its relevance and impact within the industry.

\section{\sc Projects}
{\bf Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines:}

Recent advances in Graph Neural Networks (GNNs) have changed the landscape of modern graph analytics. The complexity of GNN training and the challenges of GNN scalability has also sparked interest from the systems community, with efforts to build systems that provide higher efficiency and schemes to reduce costs. However, we observe that many such systems basically "reinvent the wheel" of much work done in the database world on scalable graph analytics engines. Further, they often tightly couple the scalability treatments of graph data processing with that of GNN training, resulting in entangled complex problems and systems that often do not scale well on one of those axes.

This paper asks a fundamental question: How far can we push existing systems for scalable graph analytics and deep learning (DL) instead of building custom GNN systems? Are compromises inevitable on scalability and/or runtimes? We propose Lotan, the first scalable and optimized data system for full-batch GNN training with \textit{decoupled scaling} that bridges the hitherto siloed worlds of graph analytics systems and DL systems. Lotan offers a series of technical innovations, including re-imagining GNN training as query plan-like dataflows, execution plan rewriting, optimized data movement between systems, a GNN-centric graph partitioning scheme, and the first known GNN model batching scheme. We prototyped Lotan on top of GraphX and PyTorch. An empirical evaluation using several real-world benchmark GNN workloads reveals a promising nuanced picture: Lotan significantly surpasses the scalability of state-of-the-art custom GNN systems, while often matching or being only slightly behind on time-to-accuracy metrics in some cases. We also show the impact of our system optimizations. Overall, our work shows that the GNN world can indeed benefit from building on top of scalable graph analytics engines. Lotan's new level of scalability can also empower new ML-oriented research on ever-larger graphs and GNNs.

\textit{Project homepage: }\texttt{https://adalabucsd.github.io/lotan.html}\\
\textit{Open-sourced release: }\texttt{https://github.com/makemebitter/lotan}

{\bf Distributed Deep Learning on Data Systems (Cerebro-DS):}

Deep learning (DL) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in RDBMSs or other data systems. The DB community has long aimed to bring machine learning (ML) to DBMS-resident data. Given past lessons from in-DBMS ML and recent advances in scalable DL systems, DBMS and cloud vendors are increasingly interested in adding more DL support for DB-resident data. In this paper, we show that there is no single ``best'' approach to achieve that goal, and an interesting tradeoff space of approaches exists. We explain four canonical approaches, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale DL workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help DBMS and cloud vendors design better DL support for DB users. The system is based on top of Postgres, Apache Spark, TensorFlow, and PyTorch. \\
\textit{Project homepage: }\texttt{https://adalabucsd.github.io/cerebro.html}\\
\textit{Open-sourced release: }\texttt{https://github.com/makemebitter/cerebro-ds}

{\bf Resource-efficient Distributed Deep Learning Model Selection and Training System (Cerebro):}

Cerebro is a layered data platform for scalable deep learning. One of the biggest challenges for scalable deep learning is model selection, which is difficult and resource-heavy. This system provides high-level APIs for deep learning model selection and optimizes distributed deep learning training for model selection workloads. It adopts a new form of parallelism called Model Hopper Parallelism (MOP) and is the resource-optimal choice. It can expedite distributed deep learning training by 3x-10x compared to Horovod and TensorFlow Parameter Server. There are also subsequent works about bridging data systems (such as Apache Spark, Distributed databases, etc.) with distributed DL systems.\\
\textit{Project homepage: }\texttt{https://adalabucsd.github.io/cerebro.html}\\
\textit{Open-sourced release: }\texttt{https://adalabucsd.github.io/cerebro-system}

{\bf Video Querying System under Unbounded Vocabulary Setting (Panorama):}

Panorama is a video querying system for object detection and classification. It is designed to tackle life-long learning or bounded vocabulary issues. Computer-vision-based video analytics systems often require \textit{update} of the vocabulary as new classes/labels need to be added from time to time. Such updates involve re-training of the deep learning model, which requires expertise and is very time-consuming. Panorama is built to partially avoid and automate this process while expediting inference speed at the same time.\\
\textit{Project homepage: }\texttt{https://adalabucsd.github.io/panorama.html}\\
\textit{Open-sourced release: }\texttt{https://github.com/makemebitter/Panorama-UCSD}

%\section{\sc Research Impact}
%Cerebro and MOP integrated into Greenplum Database and shipped by VMware \hfill 2019\\
%Code of Cerebro integrated into Apache MADlib project \hfill 2019\\

\section{\sc Academic Experience}
{\bf University of California, San Diego}, La Jolla, California USA

\vspace{-.3cm}
{\em PhD Student} \hfill {\bf 2017 - 2023}\\
Includes current Ph.D.~research, Ph.D. level coursework, and
research/consulting projects.\\
Courses taken: Machine Learning, Deep Learning, Data Mining \& Analytics, Advanced Data Analytics, Computer Vision, Database Systems, Advanced Algorithms, Advanced Compilers, Principles of Programming Languages, Advanced Operating Systems, Computer Architecture, Introduction to Robotics

{\em Collaborator on DeepPostures: a deep learning library for identifying human postures from wearable device data} \hfill {\bf 2022 - 2023}\\
Collaborated on ML model training and data science for the project. Provided software engineering support to assist public health researchers in using the library effectively. Developed and maintained the project website, authored comprehensive documentation, and created demonstration materials to showcase the library’s capabilities.  

{\em Teaching Assistant for CSE234: Data Systems for Machine Learning} \hfill {\bf Winter 2021, Winter 2023}\\
Assisted in designing and overseeing a research-focused course on machine learning systems. Mentored 12 master’s students on projects that involved implementing advanced techniques from recent research, evaluating and surveying state-of-the-art work, and exploring open-ended research questions. Delivered stand-in lectures as needed.

%\vspace{-.1cm}
{\em Teaching Assistant for DSC102: Systems for Scalable Analytics} \hfill {\bf Winter 2020}\\
Developed the inaugural set of course assignments and auto-grading scripts using Python and Bash. The assignments introduced students to key technologies, including Python Dask, Spark, AWS EC2/S3/EBS, and Kubernetes, providing hands-on experience with scalable data systems. These assignments have since become a core part of the course curriculum, utilized by over 1000 students to date.

{\bf Texas A\&M University}, College Station, Texas USA

\vspace{-.3cm}
{\em Research Intern} \hfill {\bf Summer 2015}\\
Worked on vision-based object tracking, modeling, and data analytics.
%\vspace{-.1cm}

{\bf Institute of Physics, Chinese Academy of Sciences}, Beijing, China

\vspace{-.3cm}
{\em Research Intern} \hfill {\bf Summer 2014}\\
Theoretical physics research. Particle physics.

{\bf Nankai University}, Tianjin, China

\vspace{-.3cm}
{\em Research Assistant} \hfill {\bf 2013 - 2016}\\
Theoretical physics research. Black holes, gravity, and neutrinos. Resulted in two peer-reviewed publications.



\section{\sc Publications}
\textit{Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines}\\
Yuhao Zhang and Arun Kumar\\
VLDB 2023


\textit{Some Damaging Delusions of Deep Learning Practice (and How to Avoid Them)}\\
Arun Kumar, Supun Nakandala, Yuhao Zhang\\
KDD 2021 Deep Learning Day

\textit{Distributed Deep Learning on Data Systems: A Comparative Analysis of Approaches}\\
Yuhao Zhang, Arun Kumar, Frank McQuillan, Nandish Jayaram, Nikhil Kak, Ekta Khanna, Orhan Kislal, and Domino Valdano\\
VLDB 2021

\textit{Cerebro: A Layered Data Platform for Scalable Deep Learning}\\
Arun Kumar, Supun Nakandala, Yuhao Zhang, Side Li, Advitya Gemawat, and Kabir Nagrecha\\
CIDR 2021

\textit{Cerebro: A Data System for Optimized Deep Learning Model Selection}\\
Supun Nakandala, Yuhao Zhang, and Arun Kumar\\
VLDB 2020

\textit{Panorama: A Data System for Unbounded Vocabulary Querying over Video}\\
Yuhao Zhang and Arun Kumar\\
VLDB 2020

\textit{Cerebro: Efficient and Reproducible Model Selection on Deep Learning Systems}\\
Supun Nakandala, Yuhao Zhang, and Arun Kumar\\
ACM SIGMOD 2019 DEEM Workshop

{\sc Physics Publications}

\textit{Three-generation neutrino oscillations in curved spacetime}\\
Yuhao Zhang and Xueqian Li\\
Nuclear Physics B, 2016

\textit{Self-regular black holes quantized by means of an analogue to hydrogen atoms}\\
Chang Liu, Yangang Miao, Yumei Wu, and Yuhao Zhang\\
Advances in High Energy Physics, 2016

\section{\sc Presentations}
\textit{Saturn: Efficient Multi-Model Deep Learning}\\
Poster presentation, BayLearn 2023 \hfill {\bf October 2023}\\


\textit{Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines}\\
UCSD CNS Research Review \hfill {\bf May 2023}\\
UCSD HDSI Research Review \hfill {\bf May 2023}\\
VLDB 2023 \hfill {\bf August 2023}\\

\textit{High-throughput Data Systems for Deep Learning Workloads}\\
UCSD Database Seminar \hfill {\bf April 2022}

\textit{Distributed Deep Learning on Data Systems}\\
VLDB 2021 \hfill {\bf August 2021}

\textit{A Layered Data Platform for Scalable Deep Learning}\\
UCSD Database Seminar \hfill {\bf October 2020}\\
UCSD CNS Research Review \hfill {\bf October 2020}

\vspace{-3mm}
\textit{Resource-Efficient Deep Learning Model Selection on Apache Spark}\\
Spark+AI Summit 2020 \hfill {\bf June 2020}\\
UCSD Database Seminar \hfill {\bf May 2020}

\vspace{-3mm}
\textit{Cerebro: A Data System for Optimized Deep Learning Model Selection}\\
VLDB 2020 \hfill {\bf September 2020}\\
UCSD Database Seminar \hfill {\bf June 2020}

\vspace{-3mm}
\textit{Panorama: Unbounded Vocabulary Queries on Video}\\
VLDB 2020 \hfill {\bf September 2020}\\
UCSD Database Seminar \hfill {\bf June 2020}\\
Poster presentation, UCSD CSE Research Open House \hfill {\bf January 2019}\\
UCSD Database Seminar \hfill {\bf November 2019}\\
Poster presentation, SoCal DB Day 2018 \hfill {\bf October 2018}

\vspace{-3mm}
\textit{Apache MADlib 1.17 Showcase}\\
VMware and online \hfill {\bf September 2019}\\


\section{\sc Service}

\textbf{External reviewer}: SIGMOD 2020, SIGMOD 2021, VLDB 2022

\textbf{Reviewer}: JMLR MLOSS 2022, SIGMOD 2024, VLDB 2024, VLDB Journal 2024, TKDE 2024, ACM IKDD CODS-COMAD 2024, ICDE 2026


\section{\sc Misc}

SIGMOD 2021 \textit{Students and Postdocs in DB Panel Discussion} \hfill {\bf June 2021}


\section{\sc Honors and Awards} 
ACM SIGMOD Distinguished PC Member, 2024\\
Best Thesis Award, School of Physics, Nankai University, 2016\\
Wang Kechang Scholarship for Academic Distinction, 2014\\
Gong-Neng Scholarship, 2013\\
Poling Academy Scholarship, 2012-2016


\section{\sc Technical Skills} 
\begin{list2}
\item Programming languages: Python, C/C++, Scala, Java, SQL, R
\item Data platforms: Spark, Ray, Dask, PostgreSQL, Greenplum Database, Hadoop, Hive
\item Machine learning frameworks: TensorFlow, PyTorch, Keras, xgBoost, LightGBM, Scikit-learn
\item Other data analytics packages: Pandas, NumPy, Matplotlib, OpenCV, Scikit-image, Pillow/PIL
\item Cloud/Cluster tools: AWS EC2/S3/EMR, Google Cloud, Azure, Kubernetes, Docker
\item Miscs: MPI, MATLAB, Mathematica, CERN ROOT, CERN GEANT4\\ 
\end{list2}



\end{resume}
\end{document}



